{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notations**:\n",
    "${\\Omega}$ = a sample space. An element of sample space is denoted as ${\\omega}$\n",
    "\n",
    "\n",
    "Random variables are a way to assign a real number to an event. This way we can turn events into some numerical data on which various algorithms can be applied. More formally random variable is defined as mapping ${X: \\Omega \\rightarrow R}$ that assigns a real number ${X(\\omega)}$ to every ${\\omega}$. for eg if a coin is tossed twice, the sample space is ${\\Omega = [(H,H),(H,T),(T,T),(T,H)]}$. Let's define ${X}$ as the number of times head appears, then ${X(H,H)=2}$. \n",
    "\n",
    "**Probability assigned to a random variable**:\n",
    "\n",
    "Given a random variable ${X}$ and a subset of real number line ${A}$, the probability that ${X\\in A}$ is denoted as ${P(X\\in A)}$. ${P(X\\in A)}$ = ${P\\{\\omega\\,\\in \\,\\Omega; X(\\omega)\\in A\\}}$ read as probability that X belongs to A is equal to Probability of all ${\\omega \\in \\Omega}$ such that ${X(\\omega)}$ is in ${A}$. If ${X}$ is a mapping such that it takes in an event and spits out a real number say ${r}$, then ${X^{-1}}$ is a mapping which takes in a real number ${r}$ and spits out an event ${\\omega}$ such that ${X(\\omega)=r}$. Therefore, ${P(X\\in A) = P(X^{-1}(A))}$. Since ${A}$ is a subset of real number line ${X^{-1}(A)}$ outputs a set of events which would have produced numbers in ${A}$ when input into ${X}$\n",
    "\n",
    "**Probability assigned to particular value of random variable**\n",
    "${P(X=x) = P(X^{-1}(x))= P\\{\\omega \\in \\Omega; X(\\omega)=x\\}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions Functions and Probability Functions\n",
    "\n",
    "#### Cumulative distribution functions\n",
    "A cumulative distribution function or CDF is a function ${F_{X}: R \\rightarrow [0,1]}$ defined by ${F_{X}(x) = P(X\\leq x)\n",
    "}$. In other words CDF is function which is defined on a random variable which takes as input a real number and outputs the probability of that random variable being less than or equal to that real number.\n",
    "\n",
    "Example: We toss a coin twice. ${\\Omega = [(H,T),(H,H),(T,T),(T,H)]}$. Define X as the number of heads in a single trial. ${X(T,T)=0, X(H,H)=2, X(H,T)=1, X(T,H)=1}$. ${P(X=0)= P(\\{(T,T)\\}) = 0.25, P(X=1)= P(\\{(H,T),(T,H)\\}) = 0.5, P(X=2)= P(\\{(H,H\\}) = 0.5}$.\n",
    "\n",
    "${F_{X}(1.5) = P(X \\leq 1.5) = P(X=0, X=1) = P(\\{(T,T)\\},\\{(H,T),(T,H)\\}) = 0.25+0.5 = 0.75}$. Notice that ${F_{X}(x)}$ is defined on all of number line while ${X}$ is not.\n",
    "\n",
    "**Theorem**. Let ${X}$ have CDF ${F}$ and ${Y}$ have CDF ${G}$. If ${F(x) = G(x)}$ then ${P(X\\in A) = P(Y\\in A)}$ for all A.\n",
    "\n",
    "**Proof**. It's given that ${F(x) = G(x)}$ . ${A}$ is a subset of real number line. Let's say that ${A^{-}}$ is a point just before ${A}$ and ${A^{+}}$ is a point just after ${A}$. ${P(X\\in A) = P(X\\leq A^{+}) - P(X\\leq A^{-}) = F(A^{+})- F(A^{-}) = G(A^{+})- G(A^{-}) = P(Y\\leq A^{+}) - P(Y\\leq A^{-}) = P(Y\\in A)}$\n",
    "\n",
    "**Theorem**. A function F mapping from R to ${[0,1]}$ is a CDF for some random variable iff F satisfies the following:\n",
    "\n",
    "- F is non-decreasing.i.e if ${x_{1} < x_{2}\\,then\\, F(x_{1})<F(x_{2})}$. A proof by contradiction can be that if this isn't true then P(x) for some x has to be less than 0 which isn't possible.\n",
    "- F is normalised i.e ${\\lim_{x\\to -\\infty} F(x) = 0}$ and ${\\lim_{x\\to\\infty}F(x) = 1}$. Replacing ${F(x)}$ by ${P(X\\leq x)}$ is pleasing to intutions in these two equations.\n",
    "- F is right continuous: ${F(x) = F(x^{+})}$ where ${F(x^{+}) = \\lim_{y\\to x}F(y)}$ such that ${y>x}$.\n",
    "\n",
    "**Proof**. x is a real number. Let ${y_{1},y_{2},...}$ be real numbers such that ${y_{1} > y_{2} > ...}$ and ${\\lim_{i}y_{i}}$ = x. Let ${A_{i} = (-\\infty,y_{i})}$ and ${A = (-\\infty,x)}$. We can see that ${A = \\cap_{i=1}^{\\infty}A_{i}}$. Since ${A_{1}\\supset A_{2} \\supset ...}$, ${P(\\cap_{i=1}^{\\infty}A_{i}) = \\lim_{i}P(A_{i})}$\n",
    " (if ${X\\supset Y}$ then ${P(X\\cap Y)= P(Y)}$) \n",
    "${\\therefore F(x) = P(A) = P(\\cap_{i=1}^{\\infty}A_{i}) = \\lim_{i}P(A_{i})= \\lim_{i}F(y_{i}) = F(x^{+})}$\n",
    "\n",
    "**Definition** ${X}$ is called discrete random variable if it takes countably many values in the real number line. The **probability function** or **mass function** of ${X}$ is defined in this case as ${f_{X}(x) = P(X=x)}$. Sometimes, ${f_{X}(x)}$  is abbreviated as ${f(x)}$.\n",
    "\n",
    "The definition of ${f(x)}$ implies two things: ${f(x)\\geq 0}$ for all ${x}$ and ${\\Sigma_{i}f(x_{i})=1}$ where ${x_{i}}$ is any value in the range of ${X}$.\n",
    "\n",
    "${F_{X}(x) = P(X\\leq x) = \\Sigma_{x_{i}\\leq x}f_{X}(x_{i})}$\n",
    "\n",
    "**Definition** A random variable X is called continuous if there exists a function ${f_{X}}$ such that ${f_{X}\\geq0}$ for all ${x}$, ${\\int_{-\\infty}^{\\infty}f_{X}(x)dx = 1}$ and for ${a\\geq b}$ ${P(a\\leq X\\leq b) = \\int_{a}^{b}f_{X}(x)dx}$. (A rather utilitarian definition). Notice that ${P(a\\leq X\\leq b)}$ doesn't make sense for all ${a}$ and ${b}$ if X is a discrete random variable.\n",
    "\n",
    "${F_{X}(x) = P(-\\infty\\leq X\\leq x) = \\int_{-\\infty}^{x}f_{X}(t)dt}$. This implies that ${f_{X}(x) = F'_{X}(x)}$.  ${f_{X}}$  is called the **probability density function (PDF)**.\n",
    "\n",
    "Note that ${P(X=x)}$ is 0. This makes sense since ${X}$ can take infinite number of values and probability of ${X}$ being equal to a particular ${x}$ amongst those infinite values is 0.\n",
    "\n",
    "**Caution**: ${f_{X}}$ in case of continous random variable ${X}$ can be greater than 1 unlike mass function.\n",
    "\n",
    "**Definition**: Let ${X}$ be a random variable with CDF ${F}$. Then, the **inverse CDF** or **quantile function** is defined as ${F^{-1}(q) = inf\\{x: F(x)>q\\}}$ for ${q\\in [0,1]}$ ${inf \\rightarrow \\,greatest\\, lower\\, bound}$.  ${F^{-1}(0.25)}$ is called first quartile, ${F^{-1}(0.5)}$ is called median and ${F^{-1}(0.75)}$ is called third quartile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Discrete Random Variables\n",
    "**Notation**: ${X \\sim F}$ indicates ${X}$ has distribution ${F}$.\n",
    "\n",
    "#### The Point Mass Distribution:\n",
    "${X}$ has point mass distribution at ${a}$ written as ${X\\sim\\delta_{a}}$ if ${P(X=a) = 1}$. ${f(x) = 1}$ for ${x=a}$ and 0 otherwise.\n",
    "Parameters ${\\rightarrow}$ ${a}$\n",
    "\n",
    "#### The Discrete Uniform Distribution:\n",
    "We say that ${X}$ has uniform distribution on ${(a+1,a+2,...b)}$  if ${f(x)=\\frac{1}{b-a}}$ for ${x = (a+1,a+2,...b)}$ and 0 otherwise. \n",
    "Parameters ${\\rightarrow}$ ${a\\,and\\,b}$\n",
    "\n",
    "#### The Bernoulli Distribution:\n",
    "Let's flip a biased coin that has 0 on one side and 1 on the other. Let ${X}$ denote the number which coin shows when it's flipped and fallen on the ground. ${P(X=1) = p}$ and ${P(X=0) = 1-p}$ for some ${p\\in [0,1]}$. We say that ${X\\sim Bernoulli(p)}$. ${f(x) = p^{x}(1-p)^{1-x}}$ for ${x\\in \\{0,1\\}}$. Notice that bernoulli distribution turns into uniform distribution if ${p=0.5}$.\n",
    "\n",
    "Parameters ${\\rightarrow}$ ${p}$\n",
    "\n",
    "#### The Binomial Distribution\n",
    "Let's flip a coin n times. This coin has the probability ${p}$ of showing heads and ${1-p}$ of showing tails. Define ${X}$ as the number of times heads came up in n tosses. Thus ${P(X=x) = C_{x}^{n}p^{x}(1-p)^{n-x}}$. ${X}$ is called Binomial random variable and we write ${X\\sim Binomial(n,p)}$. If ${X_{1} \\sim Binomial(n_{1},p)}$ and ${X_{2} \\sim Binomial(n_{2},p)}$ then, ${X_{1}+X_{2}\\sim Binomial(n_{1}+n_{2},p)}$. Think of this in terms of the experiment mentioned above. ${X_{1}}$,${X_{2}}$ are number of heads when that biased coin is tossed ${n_{1}}$ and ${n_{2}}$ times respectively, then ${X_{1} + X_{2}}$ is number of times heads comes up when the coin is tossed ${n_{1} + n_{2}}$ times.\n",
    "\n",
    "Parameters ${\\rightarrow}$ ${n\\,and\\,p}$\n",
    "\n",
    "#### The Geometric Distribution\n",
    "Let's take our same biased coin and toss it till the heads shows up. Let ${X}$ denote the number of times we had to toss the coin. Then, ${P(X=k) = p(1-p)^{k-1}}$. In this case we say that ${X\\sim Geom(p)}$ \n",
    "\n",
    "Parameters ${\\rightarrow}$ ${p}$.\n",
    "\n",
    "#### The Poisson Distribution\n",
    "${X\\sim Poisson(\\lambda)}$ if ${f(x)= \\large e^{-\\lambda}\\frac{\\lambda^{x}}{x!}}$ for ${x\\geq 0}$.\n",
    "\n",
    "Parameters ${\\rightarrow}$ ${\\lambda}$\n",
    "\n",
    "**Note** The value of parameters are estimated using i.i.d samples and statistical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Continuos Random Variables\n",
    "\n",
    "I used the terms ${P(X=x)}$ and ${f(x)}$ interchangeably in previous section, which can't be done in this one.\n",
    "\n",
    "#### Uniform Distribution:\n",
    "${X\\sim Uniform(a,b)}$ if ${f(x) = \\frac{1}{b-a}}$ for ${x\\in [a,b]}$ and 0 otherwise.\n",
    "\n",
    "Parameters ${\\rightarrow}$ ${a,b}$.\n",
    "\n",
    "#### Normal (Gaussian):\n",
    "${X\\sim N(\\mu, \\sigma^{2})}$ if ${f(x) = \\large \\frac{1}{\\sigma(2\\pi)^{0.5}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}}$ where ${\\mu\\in R}$ and ${\\sigma>0}$. ${X}$ has **standard Normal Distribution** if ${\\mu =0}$ and ${\\sigma=1}$.\n",
    "\n",
    "Parameters ${\\rightarrow}$ ${\\mu,\\sigma}$.\n",
    "\n",
    "#### Exponential Distribution:\n",
    "${X\\sim Exp(\\beta)}$ if ${f(x) = \\large \\frac{e^{\\large -x/\\beta}}{\\beta}}$ for ${x>0}$.\n",
    "\n",
    "Parameters ${\\rightarrow}$ ${\\beta}$.\n",
    "\n",
    "#### Gamma Distribution:\n",
    "**Gamma function** is defined as ${\\Gamma(\\alpha) = \\int_{0}^{\\infty}y^{\\alpha-1}e^{-y}dy}$ for ${x>0}$. Note that ${\\Gamma(1)=1}$. Also ${\\Gamma(\\alpha) = (\\alpha-1)\\Gamma(\\alpha-1)}$. Therefore, if ${\\alpha\\in Z}$, then ${\\Gamma(\\alpha) = (\\alpha-1)!}$.\n",
    "${X}$ is said to have Gamma Distribution denoted by ${X\\sim Gamma(\\alpha,\\beta)}$ if ${f(x)= \\large\\frac{x^{\\alpha-1}e^{\\frac{-x}{\\beta}}}{\\beta^{\\alpha}\\Gamma(\\alpha)}}$. Note that if ${\\alpha=1}$, Gamma distribution turns into exponential distribution. \n",
    "\n",
    "**Note**: If ${X_{i}\\sim Gamma(\\alpha,\\beta)}$ are independent then, ${\\sum_{i=1}^{n}X_{i}\\sim Gamma(\\sum_{i=1}^{n}\\alpha_{i},\\beta)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Distributions\n",
    "**Definition**: For a pair of discrete random variables ${X}$ and ${Y}$, the **joint mass distribution** is defined as ${f(x,y)=P(X=x\\,and\\, Y=y)}$. ${P(X=x\\,and\\,Y=y)}$ is consicely written as ${P(X=x,Y=y)}$. \n",
    "\n",
    "**Definition**: In case of continuous random variables ${X}$ and ${Y}$, ${f(x,y)}$ is a PDF for ${X,Y}$ if\n",
    "\n",
    "- ${f(x,y) \\geq 0}$ for all ${(x,y)}$\n",
    "- ${\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f(x,y)dxdy =1}$\n",
    "- ${P((X,Y)\\in A) = \\int\\int_{A}f(x,y)dxdy}$. Think of ${A}$ as an area in a plane of two coordinate axes.\n",
    "\n",
    "Joint CDF is defined as ${F_{X,Y}(x,y)= P(X\\leq x, Y\\leq y)}$\n",
    "\n",
    "### Marginal Distributions\n",
    "\n",
    "#### Discrete Case:\n",
    "If ${X}$ and ${Y}$ and random variables with joint distribution ${f(x,y)}$, then **marginal function for X** is defined as ${f_{X}(x)= P(X=x)= \\sum_{y}P(X=x, Y=y)= \\sum_{y}f(x,y)}$. Marginal function for Y is defined similarly.\n",
    "\n",
    "#### Continuous Case:\n",
    "${f_{X}(x)= \\int f(x,y)dy}$, ${f_{Y}(y)= \\int f(x,y)dx}$\n",
    "\n",
    "\n",
    "### Independent Random Variables. \n",
    "${\\newcommand{\\indep}{\\perp \\!\\!\\! \\perp}}$\n",
    "Two variables ${X}$ and ${Y}$ are independent if ${P(X\\in A,Y\\in B)= P(X\\in A)P(Y\\in B)}$ and we write ${X\\indep Y}$.\n",
    "\n",
    "**Theorem**:  ${X\\indep Y}$ iff ${f_{X,Y}(x,y)= f_{X}(x)f_{Y}(y)}$ for all  ${x\\,and\\, y}$\n",
    "\n",
    "**Theorem**: If range of ${X}$ and ${Y}$ is a (possibly infinite) rectangle then if ${f_{X,Y}(x,y)= g(x)h(y)}$, then ${X\\indep Y}$. (${g(x)\\,and\\,h(y)}$ are not necessarily PDF)\n",
    "\n",
    "### Conditional Distributions\n",
    "The **conditional probability mass function** and **conditional proabability density function** are defined as ${f_{X\\mid Y}(x\\mid y) = \\large\\frac{f(x,y)}{f_{X}(x)}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Distributions and i.i.d Samples\n",
    "\n",
    "Let ${X = (X_{1}, X_{2},...,X_{n})}$ where ${X_{1}, X_{2},...,X_{n}}$ are random variables. The pdf is denoted as ${f(x_{1}, x_{2},...,x_{n})}$. The marginals and conditionals can be defined similarly to the bivariate case. We say that ${X_{1}, X_{2},...,X_{n}}$ are independent if for every ${(A_{1}, A_{2},...,A_{n})}$, ${P(X_{1}\\in A_{1},...,P(X_{n}\\in A_{n}) = \\Pi_{i=1}^{n}P(X_{i}\\in A_{i})}$.\n",
    "\n",
    "**Definition:** If ${X_{1}, X_{2},...,X_{n}}$ are independent and each has the same marginal distribution with CDF ${F}$ then we say that ${X_{1}, X_{2},...,X_{n}}$ are i.i.d and write ${X_{1}, X_{2},...,X_{n} \\sim F}$ or we can also write ${X_{1}, X_{2},...,X_{n}\\sim f}$ where ${f}$ is the probabaility function of ${F}$. We call ${X_{1}, X_{2},...,X_{n}}$ a random sample of size ${n}$ from ${F}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Important Multivariate Distribution\n",
    "\n",
    "#### Multinomial\n",
    "Let's say a urn has balls of k colors where k is a positive integer. We pick a ball from the urn, note it's color and place it back in the urn. We do this a total of n times. Let ${X_{i}}$ denote the number of times we picked up ball with color ${i}$ where ${i\\in \\{1,2,...,k\\}}$. Let ${p_{i}}$ denote the probability of picking up ball of color ${i}$. Let ${p = (p_{1}, p_{2},...,p_{k})}$. ${\\sum_{i=1}^{k} X_{i}= n}$. Let ${X = (X_{1}, X_{2},...,X_{k})}$. We say that ${X}$ has a multinomial distribution, written as ${X\\sim Multinomial(n,p)}$. The pdf is ${f(x_{1}, x_{2},...,x_{k})= C_{x_{1}...x_{k}}^{n}p^{x_{1}}...p^{x_{k}}}$. \n",
    "\n",
    "**Lemma:** If ${X\\sim Multinomial(n,p)}$, where ${X = (X_{1}, X_{2},...,X_{k})}$ and ${p = (p_{1}, p_{2},...,p_{k})}$, then ${X_{i}\\sim Binomial(n,p_{i})}$. We can't say that ${X_{1}, X_{2},...,X_{k}}$ are i.i.d because niether are they independent nor they have the same distribution parameters.\n",
    "\n",
    "#### Multivariate Normal\n",
    "Univariate normal had parameters ${\\mu}$ and ${\\sigma}$ which were real numbers. In multivariate case ${\\mu}$ and ${\\sigma}$ are a vector and matrix respectively. Let's say ${X = (X_{1}, X_{2},...,X_{k})}$ where ${X_{1}, X_{2},...,X_{k} \\sim N(0,1)}$ and independent. Indeed, ${X_{1}, X_{2},...,X_{k}}$ are i.i.d samples of size k from a standard normal distribution. ${\\therefore f(x)= \\Pi_{i=1}^{k}f(x_{i})= \\large\\frac{1}{(2\\pi)^{0.5k}}e^{-0.5\\sum_{j=1}^{k}x_{i}^{2}}= \\large\\frac{1}{(2\\pi)^{0.5k}}e^{-0.5x^{T}x}}$.\n",
    "\n",
    "![](images/multivariate_normal.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformations of Random Variable.\n",
    "\n",
    "Suppose ${X}$ is a random variable with pdf ${f_{X}}$ and CDF ${F_{X}}$ both of which are known. Let ${Y=r(X)}$.\n",
    "${F_{Y}(y)= P(Y\\leq y)= P(r(X)\\leq y)= P(\\{x; r(x)\\leq y\\}) = \\int_{A_{y}}f_{X}(x)dx}$ where ${A_{y}= \\{x: r(x)\\leq y\\}}$. ${f_{Y}(y)}$ is then ${F'_{Y}(y)}$.\n",
    "\n",
    "for discrete case:\n",
    "${f_{Y}(y)= P(Y=y)= P(r(X)= y)= P(\\{x: r(x)=y\\})= P(X\\in r^{-1}(y))}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
