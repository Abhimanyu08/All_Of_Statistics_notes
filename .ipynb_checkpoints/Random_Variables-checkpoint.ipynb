{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notations**:\n",
    "${\\Omega}$ = a sample space. An element of sample space is denoted as ${\\omega}$\n",
    "\n",
    "\n",
    "Random variables are a way to assign a real number to an event. This way we can turn events into some numerical data on which various algorithms can be applied. More formally random variable is defined as mapping ${X: \\Omega \\rightarrow R}$ that assigns a real number ${X(\\omega)}$ to every ${\\omega}$. for eg if a coin is tossed twice, the sample space is ${\\Omega = [(H,H),(H,T),(T,T),(T,H)]}$. Let's define ${X}$ as the number of times head appears, then ${X(H,H)=2}$. \n",
    "\n",
    "**Probability assigned to a random variable**:\n",
    "\n",
    "Given a random variable ${X}$ and a subset of real number line ${A}$, the probability that ${X\\in A}$ is denoted as ${P(X\\in A)}$. ${P(X\\in A)}$ = ${P\\{\\omega\\,\\in \\,\\Omega; X(\\omega)\\in A\\}}$ read as probability that X belongs to A is equal to Probability of all ${\\omega \\in \\Omega}$ such that ${X(\\omega)}$ is in ${A}$. If ${X}$ is a mapping such that it takes in an event and spits out a real number say ${r}$, then ${X^{-1}}$ is a mapping which takes in a real number ${r}$ and spits out an event ${\\omega}$ such that ${X(\\omega)=r}$. Therefore, ${P(X\\in A) = P(X^{-1}(A))}$. Since ${A}$ is a subset of real number line ${X^{-1}(A)}$ outputs a set of events which would have produced numbers in ${A}$ when input into ${X}$\n",
    "\n",
    "**Probability assigned to particular value of random variable**\n",
    "${P(X=x) = P(X^{-1}(x))= P\\{\\omega \\in \\Omega; X(\\omega)=x\\}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions Functions and Probability Functions\n",
    "\n",
    "#### Cumulative distribution functions\n",
    "A cumulative distribution function or CDF is a function ${F_{X}: R \\rightarrow [0,1]}$ defined by ${F_{X}(x) = P(X\\leq x)\n",
    "}$. In other words CDF is function which is defined on a random variable which takes as input a real number and outputs the probability of that random variable being less than or equal to that real number.\n",
    "\n",
    "Example: We toss a coin twice. ${\\Omega = [(H,T),(H,H),(T,T),(T,H)]}$. Define X as the number of heads in a single trial. ${X(T,T)=0, X(H,H)=2, X(H,T)=1, X(T,H)=1}$. ${P(X=0)= P(\\{(T,T)\\}) = 0.25, P(X=1)= P(\\{(H,T),(T,H)\\}) = 0.5, P(X=2)= P(\\{(H,H\\}) = 0.5}$.\n",
    "\n",
    "${F_{X}(1.5) = P(X \\leq 1.5) = P(X=0, X=1) = P(\\{(T,T)\\},\\{(H,T),(T,H)\\}) = 0.25+0.5 = 0.75}$. Notice that ${F_{X}(x)}$ is defined on all of number line while ${X}$ is not.\n",
    "\n",
    "**Theorem**. Let ${X}$ have CDF ${F}$ and ${Y}$ have CDF ${G}$. If ${F(x) = G(x)}$ then ${P(X\\in A) = P(Y\\in A)}$ for all A.\n",
    "\n",
    "**Proof**. It's given that ${F(x) = G(x)}$ . ${A}$ is a subset of real number line. Let's say that ${A^{-}}$ is a point just before ${A}$ and ${A^{+}}$ is a point just after ${A}$. ${P(X\\in A) = P(X\\leq A^{+}) - P(X\\leq A^{-}) = F(A^{+})- F(A^{-}) = G(A^{+})- G(A^{-}) = P(Y\\leq A^{+}) - P(Y\\leq A^{-}) = P(Y\\in A)}$\n",
    "\n",
    "**Theorem**. A function F mapping from R to ${[0,1]}$ is a CDF for some random variable iff F satisfies the following:\n",
    "\n",
    "- F is non-decreasing.i.e if ${x_{1} < x_{2}\\,then\\, F(x_{1})<F(x_{2})}$. A proof by contradiction can be that if this isn't true then P(x) for some x has to be less than 0 which isn't possible.\n",
    "- F is normalised i.e ${\\lim_{x\\to -\\infty} F(x) = 0}$ and ${\\lim_{x\\to\\infty}F(x) = 1}$. Replacing ${F(x)}$ by ${P(X\\leq x)}$ is pleasing to intutions in these two equations.\n",
    "- F is right continuous: ${F(x) = F(x^{+})}$ where ${F(x^{+}) = \\lim_{y\\to x}F(y)}$ such that ${y>x}$.\n",
    "\n",
    "**Proof**. x is a real number. Let ${y_{1},y_{2},...}$ be real numbers such that ${y_{1} > y_{2} > ...}$ and ${\\lim_{i}y_{i}}$ = x. Let ${A_{i} = (-\\infty,y_{i})}$ and ${A = (-\\infty,x)}$. We can see that ${A = \\cap_{i=1}^{\\infty}A_{i}}$. Since ${A_{1}\\supset A_{2} \\supset ...}$, ${P(\\cap_{i=1}^{\\infty}A_{i}) = \\lim_{i}P(A_{i})}$\n",
    " (if ${X\\supset Y}$ then ${P(X\\cap Y)= P(Y)}$) \n",
    "${\\therefore F(x) = P(A) = P(\\cap_{i=1}^{\\infty}A_{i}) = \\lim_{i}P(A_{i})= \\lim_{i}F(y_{i}) = F(x^{+})}$\n",
    "\n",
    "**Definition** ${X}$ is called discrete random variable if it takes countably many values in the real number line. The **probability function** or **mass function** of ${X}$ is defined in this case as ${f_{X}(x) = P(X=x)}$. Sometimes, ${f_{X}(x)}$  is abbreviated as ${f(x)}$.\n",
    "\n",
    "The definition of ${f(x)}$ implies two things: ${f(x)\\geq 0}$ for all ${x}$ and ${\\Sigma_{i}f(x_{i})=1}$ where ${x_{i}}$ is any value in the range of ${X}$.\n",
    "\n",
    "${F_{X}(x) = P(X\\leq x) = \\Sigma_{x_{i}\\leq x}f_{X}(x_{i})}$\n",
    "\n",
    "**Definition** A random variable X is called continuous if there exists a function ${f_{X}}$ such that ${f_{X}\\geq0}$ for all ${x}$, ${\\int_{-\\infty}^{\\infty}f_{X}(x)dx = 1}$ and for ${a\\geq b}$ ${P(a\\leq X\\leq b) = \\int_{a}^{b}f_{X}(x)dx}$. (A rather utilitarian definition). Notice that ${P(a\\leq X\\leq b)}$ doesn't make sense for all ${a}$ and ${b}$ if X is a discrete random variable.\n",
    "\n",
    "${F_{X}(x) = P(-\\infty\\leq X\\leq x) = \\int_{-\\infty}^{x}f_{X}(t)dt}$. This implies that ${f_{X}(x) = F'_{X}(x)}$.  ${f_{X}}$  is called the **probability density function (PDF)**.\n",
    "\n",
    "Note that ${P(X=x)}$ is 0. This makes sense since ${X}$ can take infinite number of values and probability of ${X}$ being equal to a particular ${x}$ amongst those infinite values is 0.\n",
    "\n",
    "**Caution**: ${f_{X}}$ in case of continous random variable ${X}$ can be greater than 1 unlike mass function.\n",
    "\n",
    "**Definition**: Let ${X}$ be a random variable with CDF ${F}$. Then, the **inverse CDF** or **quantile function** is defined as ${F^{-1}(q) = inf\\{x: F(x)>q\\}}$ for ${q\\in [0,1]}$ ${inf \\rightarrow \\,greatest\\, lower\\, bound}$.  ${F^{-1}(0.25)}$ is called first quartile, ${F^{-1}(0.5)}$ is called median and ${F^{-1}(0.75)}$ is called third quartile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Discrete Random Variables\n",
    "**Notation**: ${X \\sim F}$ indicates ${X}$ has distribution ${F}$.\n",
    "\n",
    "#### The Point Mass Distribution:\n",
    "${X}$ has point mass distribution at ${a}$ written as ${X\\sim\\delta_{a}}$ if ${P(X=a) = 1}$. ${f(x) = 1}$ for ${x=a}$ and 0 otherwise.\n",
    "Parameters ${\\rightarrow}$ ${a}$\n",
    "\n",
    "#### The Discrete Uniform Distribution:\n",
    "We say that ${X}$ has uniform distribution on ${(a+1,a+2,...b)}$  if ${f(x)=\\frac{1}{b-a}}$ for ${x = (a+1,a+2,...b)}$ and 0 otherwise. \n",
    "Parameters ${\\rightarrow}$ ${a\\,and\\,b}$\n",
    "\n",
    "#### The Bernoulli Distribution:\n",
    "Let's flip a biased coin that has 0 on one side and 1 on the other. Let ${X}$ denote the number which coin shows when it's flipped and fallen on the ground. ${P(X=1) = p}$ and ${P(X=0) = 1-p}$ for some ${p\\in [0,1]}$. We say that ${X\\sim Bernoulli(p)}$. ${f(x) = p^{x}(1-p)^{1-x}}$ for ${x\\in \\{0,1\\}}$. Notice that bernoulli distribution turns into uniform distribution if ${p=0.5}$.\n",
    "\n",
    "Parameters ${\\rightarrow}$ ${p}$\n",
    "\n",
    "#### The Binomial Distribution\n",
    "Let's flip a coin n times. This coin has the probability ${p}$ of showing heads and ${1-p}$ of showing tails. Define ${X}$ as the number of times heads came up in n tosses. Thus ${P(X=x) = C_{x}^{n}p^{x}(1-p)^{n-x}}$. ${X}$ is called Binomial random variable and we write ${X\\sim Binomial(n,p)}$. If ${X_{1} \\sim Binomial(n_{1},p)}$ and ${X_{1} \\sim Binomial(n_{2},p)}$ then, ${X_{1}+X_{2}\\sim Binomial(n_{1}+n_{2},p)}$. Think of this in terms of the experiment mentioned above. ${X_{1}}$,${X_{2}}$ are number of heads when that biased coin is tossed ${n_{1}}$ and ${n_{2}}$ times respectively, then ${X_{1} + X_{2}}$ is number of times heads comes up when the coin is tossed ${n_{1} + n_{2}}$ times.\n",
    "\n",
    "Parameters ${\\rightarrow}$ ${n\\,and\\,p}$\n",
    "\n",
    "#### The Geometric Distribution\n",
    "Let's take our same biased coin and toss it till the heads shows up. Let ${X}$ denote the number of times we had to toss the coin. Then, ${P(X=k) = p(1-p)^{k-1}}$. In this case we say that ${X\\sim Geom(p)}$ \n",
    "\n",
    "Parameters ${\\rightarrow}$ ${p}$.\n",
    "\n",
    "#### The Poisson Distribution\n",
    "${X\\sim Poisson(\\lambda)}$ if ${f(x)= \\large e^{-\\lambda}\\frac{\\lambda^{x}}{x!}}$ for ${x\\geq 0}$.\n",
    "\n",
    "Parameters ${\\rightarrow}$ ${\\lambda}$\n",
    "\n",
    "**Note** The value of parameters are estimated using i.i.d samples and statistical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Continuos Random Variables\n",
    "\n",
    "I used the terms ${P(X=x)}$ and ${f(x)}$ interchangeably in previous section, which can't be done in this one.\n",
    "\n",
    "#### Uniform Distribution:\n",
    "${X\\sim Uniform(a,b)}$ if ${f(x) = \\frac{1}{b-a}}$ for ${x\\in [a,b]}$ and 0 otherwise.\n",
    "\n",
    "Parameters ${\\rightarrow}$ ${a,b}$.\n",
    "\n",
    "#### Normal (Gaussian):\n",
    "${X\\sim N(\\mu, \\sigma^{2})}$ if ${f(x) = \\large \\frac{1}{\\sigma(2\\pi)^{0.5}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}}$ where ${\\mu\\in R}$ and ${\\sigma>0}$. ${X}$ has **standard Normal Distribution** if ${\\mu =0}$ and ${\\sigma=1}$.\n",
    "\n",
    "Parameters ${\\rightarrow}$ ${\\mu,\\sigma}$.\n",
    "\n",
    "#### Exponential Distribution:\n",
    "${X\\sim Exp(\\beta)}$ if ${f(x) = \\large \\frac{e^{\\large -x/\\beta}}{\\beta}}$ for ${x>0}$.\n",
    "\n",
    "Parameters ${\\rightarrow}$ ${\\beta}$.\n",
    "\n",
    "#### Gamma Distribution:\n",
    "**Gamma function** is defined as ${\\Gamma(\\alpha) = \\int_{0}^{\\infty}y^{\\alpha-1}e^{-y}dy}$ for ${x>0}$. Note that ${\\Gamma(1)=1}$. Also ${\\Gamma(\\alpha) = (\\alpha-1)\\Gamma(\\alpha-1)}$. Therefore, if ${\\alpha\\in I}$, then ${\\Gamma(\\alpha) = (\\alpha-1)!}$.\n",
    "\n",
    "${X}$ is said to have Gamma Distribution denoted by ${X\\sim Gamma(\\alpha,\\beta)}$ if ${f(x)= \\large\\frac{x^{\\alpha-1}e^{\\frac{-x}{\\beta}}}{\\beta^{\\alpha}\\Gamma(\\alpha)}}$. Note that if ${\\alpha=1}$, Gamma distribution turns into exponential distribution. \n",
    "\n",
    "**Note**: If ${X_{i}}$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
